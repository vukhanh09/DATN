{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71cd64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sparkmeasure import StageMetrics,TaskMetrics\n",
    "from pyspark.sql.functions import col,from_json,udf\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,MapType,FloatType,ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e74e82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"testing\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1024m\").\\\n",
    "        config(\"spark.jars.packages\", \"ch.cern.sparkmeasure:spark-measure_2.12:0.17\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc405665",
   "metadata": {},
   "outputs": [],
   "source": [
    "stagemetrics = StageMetrics(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23d41345",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('hdfs://namenode:9000/TikiCleaned/metaData').limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17e3791c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=====================================================> (146 + 2) / 149]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|category_id|count|\n",
      "+-----------+-----+\n",
      "|       8322|88863|\n",
      "|       1815| 5816|\n",
      "|       1882| 5321|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.groupby('category_id').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7413692",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.createOrReplaceTempView('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953fb17b",
   "metadata": {},
   "source": [
    "# Storage space in variety of format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d869022",
   "metadata": {},
   "source": [
    "## Write statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b7df0",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e312ee10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 13:39:14 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:39:52 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:40:33 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:41:10 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:41:48 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:42:25 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:43:02 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:43:39 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:44:20 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:45:00 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "csv_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    data.write.partitionBy(\"category_id\").option(\"header\",True).mode('overwrite').csv('hdfs://namenode:9000/testing/data/csv')\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    csv_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae30c6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41895, 37839, 40360, 36992, 37097, 37076, 36586, 36807, 40810, 40222]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53413412",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f6c55a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 12:51:50 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:52:27 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:52:59 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:53:32 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:54:06 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:54:38 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:55:13 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:55:45 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:56:18 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:56:51 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "parquet_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    data.write.partitionBy(\"category_id\").mode('overwrite').parquet('hdfs://namenode:9000/testing/data/parquet')\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    parquet_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b85363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33887, 36467, 32081, 32506, 33377, 32419, 33940, 31878, 33363, 32263]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de702f52",
   "metadata": {},
   "source": [
    "### Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e49e4bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 12:57:34 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:58:11 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:58:48 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 12:59:29 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:00:06 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:00:47 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:01:25 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:02:02 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:02:42 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:03:20 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "json_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    data.write.partitionBy(\"category_id\").mode('overwrite').json('hdfs://namenode:9000/testing/data/json')\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    json_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6364e35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42320, 36728, 37449, 40406, 37107, 40874, 37085, 37211, 39872, 37668]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fef196",
   "metadata": {},
   "source": [
    "### Orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59dd1391",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 13:03:53 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:04:25 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:05:00 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:05:32 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:06:04 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:06:37 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:07:10 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:07:43 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:08:15 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:08:52 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "orc_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    data.write.partitionBy(\"category_id\").mode('overwrite').orc('hdfs://namenode:9000/testing/data/orc')\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    orc_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a97ec31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32334, 32013, 34310, 32299, 31692, 33027, 32665, 32165, 32108, 36520]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orc_elapsedTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9637948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.write.mode('overwrite').csv('hdfs://namenode:9000/testing/data/csv')\n",
    "# data.write.mode('overwrite').parquet('hdfs://namenode:9000/testing/data/parquet')\n",
    "# data.write.mode('overwrite').json('hdfs://namenode:9000/testing/data/json')\n",
    "# data.write.mode('overwrite').orc('hdfs://namenode:9000/testing/data/orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190624a5",
   "metadata": {},
   "source": [
    "## Read statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a866b",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "89dc5e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 13:45:01 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:45:02 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:45:03 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:45:04 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:45:05 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:45:06 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:45:07 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:45:08 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:45:09 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:45:10 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "csv_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.read.option(\"header\", \"true\").csv('hdfs://namenode:9000/testing/data/csv').take(10000)\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    csv_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "676a645c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[615, 555, 556, 643, 586, 707, 775, 551, 579, 570]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509841ef",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a52a687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 13:12:04 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:12:05 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:12:05 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:12:06 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:12:07 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:12:08 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:12:08 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:12:09 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:12:10 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:12:10 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "parquet_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.read.parquet('hdfs://namenode:9000/testing/data/parquet').take(10000)\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    parquet_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b1463e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[273, 244, 298, 257, 271, 284, 196, 178, 213, 184]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b477bfd",
   "metadata": {},
   "source": [
    "### Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae917918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 13:12:41 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:13:06 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:13:30 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:13:53 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:14:17 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:14:40 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:15:03 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:15:27 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:15:52 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:16:15 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "json_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.read.json('hdfs://namenode:9000/testing/data/json').take(10000)\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    json_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "511eb55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25980, 24505, 23554, 23570, 22885, 22651, 22959, 23929, 24180, 22880]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1494c0d3",
   "metadata": {},
   "source": [
    "### Orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23f59f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 13:16:40 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:16:40 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:16:41 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:16:42 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:16:42 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:16:43 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:16:43 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:16:44 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:16:44 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:16:45 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "orc_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.read.orc('hdfs://namenode:9000/testing/data/orc').take(10000)\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    orc_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b142852a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[207, 212, 183, 181, 203, 186, 163, 155, 165, 166]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orc_elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5eaabf",
   "metadata": {},
   "source": [
    "# Filter performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a86b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_csv = spark.read.option(\"header\", \"true\").csv('hdfs://namenode:9000/testing/data/csv')\n",
    "df_parquet = spark.read.parquet('hdfs://namenode:9000/testing/data/parquet')\n",
    "df_json = spark.read.json('hdfs://namenode:9000/testing/data/json')\n",
    "df_orc = spark.read.orc('hdfs://namenode:9000/testing/data/orc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a61f6b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 15:16:47 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df_csv.createOrReplaceTempView('df_csv')\n",
    "df_parquet.createOrReplaceTempView('df_parquet')\n",
    "df_json.createOrReplaceTempView('df_json')\n",
    "df_orc.createOrReplaceTempView('df_orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2b7d7",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6ba1f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 13:50:02 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:02 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:03 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:03 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:04 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:04 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:05 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:05 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:06 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:06 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "csv_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.sql('''\n",
    "                select * from df_csv\n",
    "                where price > 100000\n",
    "            ''').take(1000)\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    csv_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d74396c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[297, 369, 316, 314, 321, 316, 327, 319, 307, 290]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af39ac",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1cdc2aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 13:50:31 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:32 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:32 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:32 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:32 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:33 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:33 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:33 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:33 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:50:33 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "parquet_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.sql('''\n",
    "                select * from df_parquet\n",
    "                where price > 100000\n",
    "            ''').take(1000)\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    parquet_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "88f837bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[92, 75, 40, 40, 44, 38, 51, 55, 37, 46]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_elapsedTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "93040fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 13:51:07 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:08 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:08 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:08 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:09 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:09 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:10 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:10 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:11 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:11 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "json_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.sql('''\n",
    "                select * from df_json\n",
    "                where price > 100000\n",
    "            ''').take(1000)\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    json_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8c7eb699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[277, 220, 237, 231, 283, 264, 281, 291, 264, 295]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_elapsedTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c4143d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/08 13:51:45 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:45 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:45 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:45 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:45 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:46 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:46 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:46 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:46 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n",
      "23/01/08 13:51:47 WARN StageMetrics: Stage metrics data refreshed into temp view PerfStageMetrics\n"
     ]
    }
   ],
   "source": [
    "orc_elapsedTime = []\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.sql('''\n",
    "                select * from df_orc\n",
    "                where price > 100000\n",
    "            ''').take(1000)\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    orc_elapsedTime.append(elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0a4a97f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[122, 48, 116, 37, 49, 39, 52, 39, 47, 42]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orc_elapsedTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf61c45",
   "metadata": {},
   "source": [
    " # Aggregate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992518df",
   "metadata": {},
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = defaultdict(list)\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.sql('''\n",
    "                select category_id,sum(price) total_price from df_csv\n",
    "                group by category_id\n",
    "            ''').collect()\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    executorCpuTime = int(stagemetrics.report().replace(' ','').split('\\n')[9].split('>')[1].split('(')[0])\n",
    "    jvmGCTime = int(stagemetrics.report().replace(' ','').split('\\n')[13].split('>')[1].split('(')[0])\n",
    "    \n",
    "    \n",
    "    agg_dict['elapsedTime'].append(elapsedTime)\n",
    "    agg_dict['executorCpuTime'].append(executorCpuTime)\n",
    "    agg_dict['jvmGCTime'].append(jvmGCTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "19a8a4c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'elapsedTime': [3720,\n",
       "              3542,\n",
       "              3656,\n",
       "              3435,\n",
       "              3686,\n",
       "              3680,\n",
       "              3629,\n",
       "              3745,\n",
       "              3487,\n",
       "              3204],\n",
       "             'executorCpuTime': [6212,\n",
       "              5912,\n",
       "              6081,\n",
       "              5721,\n",
       "              6194,\n",
       "              6233,\n",
       "              6108,\n",
       "              6319,\n",
       "              5988,\n",
       "              5413],\n",
       "             'jvmGCTime': [52, 50, 83, 50, 57, 51, 71, 44, 49, 47]})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6b2d4f",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a4e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = defaultdict(list)\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.sql('''\n",
    "                select category_id,sum(price) total_price from df_parquet\n",
    "                group by category_id\n",
    "            ''').collect()\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    executorCpuTime = int(stagemetrics.report().replace(' ','').split('\\n')[9].split('>')[1].split('(')[0])\n",
    "    jvmGCTime = int(stagemetrics.report().replace(' ','').split('\\n')[13].split('>')[1].split('(')[0])\n",
    "    \n",
    "    \n",
    "    agg_dict['elapsedTime'].append(elapsedTime)\n",
    "    agg_dict['executorCpuTime'].append(executorCpuTime)\n",
    "    agg_dict['jvmGCTime'].append(jvmGCTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eceecde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'elapsedTime': [444, 456, 526, 466, 468, 451, 441, 425, 516, 474],\n",
       "             'executorCpuTime': [263,\n",
       "              256,\n",
       "              290,\n",
       "              278,\n",
       "              278,\n",
       "              264,\n",
       "              261,\n",
       "              246,\n",
       "              297,\n",
       "              284],\n",
       "             'jvmGCTime': [4, 4, 4, 4, 0, 7, 6, 7, 5, 0]})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cab630",
   "metadata": {},
   "source": [
    "## Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = defaultdict(list)\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.sql('''\n",
    "                select category_id,sum(price) total_price from df_json\n",
    "                group by category_id\n",
    "            ''').collect()\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    executorCpuTime = int(stagemetrics.report().replace(' ','').split('\\n')[9].split('>')[1].split('(')[0])\n",
    "    jvmGCTime = int(stagemetrics.report().replace(' ','').split('\\n')[13].split('>')[1].split('(')[0])\n",
    "    \n",
    "    \n",
    "    agg_dict['elapsedTime'].append(elapsedTime)\n",
    "    agg_dict['executorCpuTime'].append(executorCpuTime)\n",
    "    agg_dict['jvmGCTime'].append(jvmGCTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "31f93381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'elapsedTime': [1973,\n",
       "              2016,\n",
       "              1959,\n",
       "              2068,\n",
       "              1809,\n",
       "              2090,\n",
       "              1799,\n",
       "              2024,\n",
       "              2020,\n",
       "              2100],\n",
       "             'executorCpuTime': [2868,\n",
       "              3178,\n",
       "              2836,\n",
       "              3196,\n",
       "              2740,\n",
       "              3274,\n",
       "              2713,\n",
       "              3129,\n",
       "              3034,\n",
       "              3256],\n",
       "             'jvmGCTime': [17, 9, 0, 4, 13, 10, 13, 7, 8, 8]})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836be57",
   "metadata": {},
   "source": [
    "## Orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d48be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict = defaultdict(list)\n",
    "for i in range(10):\n",
    "    stagemetrics.begin()\n",
    "    tmp = spark.sql('''\n",
    "                select category_id,sum(price) total_price from df_orc\n",
    "                group by category_id\n",
    "            ''').collect()\n",
    "    stagemetrics.end()\n",
    "    \n",
    "    elapsedTime = int(stagemetrics.report().replace(' ','').split('\\n')[6].split('>')[1].split('(')[0])\n",
    "    executorCpuTime = int(stagemetrics.report().replace(' ','').split('\\n')[9].split('>')[1].split('(')[0])\n",
    "    jvmGCTime = int(stagemetrics.report().replace(' ','').split('\\n')[13].split('>')[1].split('(')[0])\n",
    "    \n",
    "    \n",
    "    agg_dict['elapsedTime'].append(elapsedTime)\n",
    "    agg_dict['executorCpuTime'].append(executorCpuTime)\n",
    "    agg_dict['jvmGCTime'].append(jvmGCTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "715a4ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'elapsedTime': [443, 466, 504, 518, 564, 509, 442, 514, 506, 433],\n",
       "             'executorCpuTime': [269,\n",
       "              258,\n",
       "              260,\n",
       "              271,\n",
       "              290,\n",
       "              259,\n",
       "              237,\n",
       "              276,\n",
       "              255,\n",
       "              239],\n",
       "             'jvmGCTime': [0, 18, 9, 11, 26, 11, 12, 31, 11, 0]})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6dc8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
