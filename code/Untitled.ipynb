{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8031ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from json import dumps\n",
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ebb3c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "kafka = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\"\n",
    "spark = \"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.1.0\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\"--packages {},{} pyspark-shell\".format(kafka, spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0827ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-96c23799-e411-41dc-b0e1-d22bb163df4f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.1.0 in central\n",
      "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
      "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 in central\n",
      "\tfound com.101tec#zkclient;0.3 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.0/spark-sql-kafka-0-10_2.12-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0!spark-sql-kafka-0-10_2.12.jar (926ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.1.0/spark-streaming-kafka-0-8_2.11-2.1.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-streaming-kafka-0-8_2.11;2.1.0!spark-streaming-kafka-0-8_2.11.jar (857ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.0.0/spark-token-provider-kafka-0-10_2.12-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0!spark-token-provider-kafka-0-10_2.12.jar (298ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.4.1!kafka-clients.jar (7977ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (529ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (208ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.4-3/zstd-jni-1.4.4-3.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.4-3!zstd-jni.jar (14459ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (2134ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.5/snappy-java-1.1.7.5.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.5!snappy-java.jar(bundle) (6198ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (295ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka_2.11/0.8.2.1/kafka_2.11-0.8.2.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka_2.11;0.8.2.1!kafka_2.11.jar (12242ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.2/scala-xml_2.11-1.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-xml_2.11;1.0.2!scala-xml_2.11.jar(bundle) (3279ms)\n",
      "downloading https://repo1.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar ...\n",
      "\t[SUCCESSFUL ] com.yammer.metrics#metrics-core;2.2.0!metrics-core.jar (659ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.2/scala-parser-combinators_2.11-1.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2!scala-parser-combinators_2.11.jar(bundle) (2043ms)\n",
      "downloading https://repo1.maven.org/maven2/com/101tec/zkclient/0.3/zkclient-0.3.jar ...\n",
      "\t[SUCCESSFUL ] com.101tec#zkclient;0.3!zkclient.jar (454ms)\n",
      "downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n",
      "\t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (2265ms)\n",
      ":: resolution report :: resolve 27878ms :: artifacts dl 54846ms\n",
      "\t:: modules in use:\n",
      "\tcom.101tec#zkclient;0.3 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.1.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.16 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\torg.apache.kafka#kafka-clients;0.8.2.1 by [org.apache.kafka#kafka-clients;2.4.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   16  |   16  |   2   ||   16  |   16  |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/apache/18/apache-18.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/spark/spark-parent_2.12/3.0.0/spark-parent_2.12-3.0.0.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/slf4j/slf4j-parent/1.7.30/slf4j-parent-1.7.30.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/sonatype/oss/oss-parent/9/oss-parent-9.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/apache/21/apache-21.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/commons/commons-parent/48/commons-parent-48.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/apache/14/apache-14.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/spark/spark-parent_2.11/2.1.0/spark-parent_2.11-2.1.0.jar\n",
      "\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/com/yammer/metrics/metrics-parent/2.2.0/metrics-parent-2.2.0.jar\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-96c23799-e411-41dc-b0e1-d22bb163df4f\n",
      "\tconfs: [default]\n",
      "\t16 artifacts copied, 0 already retrieved (16207kB/45ms)\n",
      "22/10/20 17:15:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fc6a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BROKER='kafka-1:9092'\n",
    "KAFKA_TOPIC='Product'\n",
    "kafkaMessages = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", KAFKA_BROKER) \\\n",
    "  .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a949fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = kafkaMessages.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b69bbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/20 17:21:32 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-06945b3e-cadc-4b2f-b756-fbacabeb5aaa. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "rawQuery = message \\\n",
    "        .writeStream \\\n",
    "        .queryName(\"dtraw3\")\\\n",
    "        .format(\"memory\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58290910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|              value|\n",
      "+-------------------+\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "|{\"name\": \"Khanhvx\"}|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM dtraw3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f3dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'Test',\n",
    "     bootstrap_servers=['kafka-1:9092'],\n",
    "     auto_offset_reset='earliest',\n",
    "     enable_auto_commit=True,\n",
    "     group_id='my-group',\n",
    "     value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd8c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "for message in consumer:\n",
    "    print(message.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a6616b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
