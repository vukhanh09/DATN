{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d024748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from json import dumps\n",
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470f22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "kafka = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\"\n",
    "spark = \"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.1.0\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\"--packages {},{} pyspark-shell\".format(kafka, spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f42b846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-421af31b-e2ca-411c-b7f5-d56cabd1fe54;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.1.0 in central\n",
      "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
      "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 in central\n",
      "\tfound com.101tec#zkclient;0.3 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      ":: resolution report :: resolve 4708ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.101tec#zkclient;0.3 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.1.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.16 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\torg.apache.kafka#kafka-clients;0.8.2.1 by [org.apache.kafka#kafka-clients;2.4.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   1   |   1   |   2   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tSERVER ERROR: Bad Gateway url=https://dl.bintray.com/spark-packages/maven/org/apache/spark/spark-parent_2.11/2.1.0/spark-parent_2.11-2.1.0.jar\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-421af31b-e2ca-411c-b7f5-d56cabd1fe54\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/9ms)\n",
      "22/10/24 17:06:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b979126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BROKER1 ='kafka-1:9092'\n",
    "KAFKA_BROKER2 ='kafka-2:9092'\n",
    "KAFKA_TOPIC='tt'\n",
    "kafkaMessages = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", f'{KAFKA_BROKER1},{KAFKA_BROKER2}') \\\n",
    "  .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"includeHeaders\", \"true\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a081e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fff65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "schema = StructType([ \n",
    "    StructField(\"id\",StringType(),True), \n",
    "    StructField(\"master_id\",StringType(),True), \n",
    "    StructField(\"sku\",StringType(),True), \n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02f973f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='1379981', master_id='1379981', sku='2411205315673')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.withColumn(\"jsonData\",from_json(col(\"value\"),schema)) \\\n",
    "                   .select(\"jsonData.*\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d428af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fe2d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet('hdfs://namenode:9000/tiki/Product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d69f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c8ec404",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = data.select('value').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4eff588",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = sample_data[0].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "accee00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1d2dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = loads(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd65a073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id <class 'int'>\n",
      "master_id <class 'int'>\n",
      "sku <class 'str'>\n",
      "name <class 'str'>\n",
      "url_key <class 'str'>\n",
      "url_path <class 'str'>\n",
      "short_url <class 'str'>\n",
      "type <class 'str'>\n",
      "book_cover <class 'NoneType'>\n",
      "short_description <class 'str'>\n",
      "price <class 'int'>\n",
      "list_price <class 'int'>\n",
      "original_price <class 'int'>\n",
      "badges <class 'list'>\n",
      "badges_new <class 'list'>\n",
      "discount <class 'int'>\n",
      "discount_rate <class 'int'>\n",
      "rating_average <class 'float'>\n",
      "review_count <class 'int'>\n",
      "review_text <class 'str'>\n",
      "favourite_count <class 'int'>\n",
      "thumbnail_url <class 'str'>\n",
      "has_ebook <class 'bool'>\n",
      "inventory_status <class 'str'>\n",
      "inventory_type <class 'str'>\n",
      "productset_group_name <class 'str'>\n",
      "is_fresh <class 'bool'>\n",
      "seller <class 'NoneType'>\n",
      "is_flower <class 'bool'>\n",
      "has_buynow <class 'bool'>\n",
      "is_gift_card <class 'bool'>\n",
      "salable_type <class 'NoneType'>\n",
      "data_version <class 'int'>\n",
      "day_ago_created <class 'int'>\n",
      "all_time_quantity_sold <class 'int'>\n",
      "meta_title <class 'str'>\n",
      "meta_description <class 'str'>\n",
      "meta_keywords <class 'str'>\n",
      "is_baby_milk <class 'bool'>\n",
      "is_acoholic_drink <class 'bool'>\n",
      "description <class 'str'>\n",
      "images <class 'list'>\n",
      "warranty_policy <class 'NoneType'>\n",
      "brand <class 'dict'>\n",
      "current_seller <class 'dict'>\n",
      "other_sellers <class 'list'>\n",
      "seller_specifications <class 'list'>\n",
      "specifications <class 'list'>\n",
      "product_links <class 'list'>\n",
      "services_and_promotions <class 'list'>\n",
      "promitions <class 'list'>\n",
      "stock_item <class 'dict'>\n",
      "quantity_sold <class 'dict'>\n",
      "categories <class 'dict'>\n",
      "breadcrumbs <class 'list'>\n",
      "installment_info <class 'NoneType'>\n",
      "installment_info_v2 <class 'NoneType'>\n",
      "video_url <class 'str'>\n",
      "is_seller_in_chat_whitelist <class 'bool'>\n",
      "inventory <class 'dict'>\n",
      "warranty_info <class 'list'>\n",
      "return_and_exchange_policy <class 'str'>\n",
      "asa_cashback_widget <class 'dict'>\n",
      "benefits <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for key in data_sample:\n",
    "    print(key,type(data_sample[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75ec670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fileStream = message.writeStream \\\n",
    "#                       .trigger(processingTime='60 seconds')\\\n",
    "#                       .queryName(\"Persist the processed data\") \\\n",
    "#                       .outputMode(\"append\") \\\n",
    "#                       .format(\"parquet\") \\\n",
    "#                       .option(\"path\", \"hdfs://namenode:9000/tiki/Product/\") \\\n",
    "#                       .option(\"checkpointLocation\", \"hdfs://namenode:9000/tiki/checkpoints\") \\\n",
    "#                       .start() \\\n",
    "#                       .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd1dcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafkaMessages.writeStream.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149e0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = kafkaMessages.selectExpr(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c318e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/24 17:07:18 WARN StreamingQueryManager: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c5165fc0-dd6a-411d-94c8-35843ff0ee04. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    }
   ],
   "source": [
    "rawQuery = message \\\n",
    "        .writeStream \\\n",
    "        .queryName(\"dasd\")\\\n",
    "        .format(\"memory\")\\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7895d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='{\"id\": 53789448, \"master_id\": 53789448}')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT cast(value as string) FROM dasd limit 1\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c588dfdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fileStream \u001b[38;5;241m=\u001b[39m \u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m5 seconds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPersist the processed data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://namenode:9000/tiki/tt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://namenode:9000/tiki/checkpointss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/streaming.py:103\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m/usr/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fileStream = message.writeStream \\\n",
    "                      .trigger(processingTime='5 seconds')\\\n",
    "                      .queryName(\"Persist the processed data\") \\\n",
    "                      .outputMode(\"append\") \\\n",
    "                      .format(\"parquet\") \\\n",
    "                      .option(\"path\", f\"hdfs://namenode:9000/tiki/tt\") \\\n",
    "                      .option(\"checkpointLocation\", \"hdfs://namenode:9000/tiki/checkpointss\") \\\n",
    "                      .start() \\\n",
    "                      .awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e14ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
