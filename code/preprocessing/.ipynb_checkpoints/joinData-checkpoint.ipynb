{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c53d28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyspark.sql.functions import col,from_json,udf\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,MapType,FloatType,ArrayType,BooleanType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b4bd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/27 06:22:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"process-data\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1024m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9ae212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_product = spark.read.parquet('hdfs://namenode:9000/TikiCleaned/Product')\n",
    "df_comment = spark.read.parquet('hdfs://namenode:9000/TikiCleaned/Comment')\n",
    "df_shop = spark.read.parquet('hdfs://namenode:9000/TikiCleaned/ShopInfo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa1da056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product.createOrReplaceTempView(\"product\")\n",
    "df_comment.createOrReplaceTempView(\"comment\")\n",
    "df_shop.createOrReplaceTempView(\"shop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "044b5480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comment count: 4243515 \n",
      "product count: 121040 \n",
      "shop count: 9945\n"
     ]
    }
   ],
   "source": [
    "print(\"comment count: {} \\nproduct count: {} \\nshop count: {}\".format(df_comment.count(),df_product.count(),\n",
    "                                       df_shop.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec914969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, master_id: int, sku: string, price: int, list_price: int, original_price: int, discount: int, discount_rate: float, rating_average: float, review_count: int, productset_group_name: string, all_time_quantity_sold: int, name: string, short_description: string, clean_name: string, clean_description: string, clean_specifications: string, category_name: string, seller_id: string, seller_name: string, seller_store_id: string, spid: int, category_id: int]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6e9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_product_shop = spark.sql(\"\"\"\n",
    "    select p.id p_id, p.master_id,p.price,p.list_price,p.original_price,p.discount,p.discount_rate,\n",
    "    p.productset_group_name,p.all_time_quantity_sold,p.name p_name,p.short_description,p.clean_name p_clean_name,\n",
    "    p.clean_description,p.clean_specifications,\n",
    "    p.category_name,p.category_id,\n",
    "    p.seller_id, s.store_id,s.name s_name,s.icon,s.url,s.is_official,s.store_level,s.is_followed,\n",
    "    s.avg_rating_point,s.review_count, s.total_follower,s.days_since_joined\n",
    "    from product p\n",
    "    join shop s on p.seller_id = s.id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f826f69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/27 06:23:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_product_shop.write.partitionBy(\"category_id\").mode('append').parquet('hdfs://namenode:9000/TikiCleaned/Product_Shop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e03a4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = spark.sql(\"\"\"\n",
    "    select p.id p_id, p.master_id,p.price,p.list_price,p.original_price,p.discount,p.discount_rate,\n",
    "    p.productset_group_name,p.all_time_quantity_sold,p.name p_name,p.short_description,p.clean_name p_clean_name,\n",
    "    p.clean_description,p.clean_specifications,\n",
    "    p.category_name,p.category_id,\n",
    "    p.seller_id, s.store_id,s.name s_name,s.icon,s.url,s.is_official,s.store_level,s.is_followed,\n",
    "    s.avg_rating_point,s.review_count, s.total_follower,s.days_since_joined, \n",
    "    c.id c_id,c.content, c.clean_content,c.thank_count,c.customer_id,c.rating,\n",
    "    c.customer_full_name,c.purchased_at,c.date_purchased_at,c.review_created_date,\n",
    "    c.delivery_date,c.review_after_delivery,c.sentiment,c.lable\n",
    "    from product p\n",
    "    join shop s on p.seller_id = s.id\n",
    "    join comment c on c.product_id = p.id and p.seller_id = c.seller_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78b6a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_all.write.partitionBy(\"category_id\").mode('append').parquet('hdfs://namenode:9000/TikiCleaned/metaData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8d8526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89965a23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
