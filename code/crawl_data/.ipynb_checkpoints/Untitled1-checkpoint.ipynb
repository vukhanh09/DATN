{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "876b4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "\n",
    "from json import dumps\n",
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "\n",
    "# define environment kafka streaming\n",
    "kafka = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0\"\n",
    "spark = \"org.apache.spark:spark-streaming-kafka-0-8_2.11:2.1.0\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\"--packages {},{} pyspark-shell\".format(kafka, spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b8889e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1e6b4e8d-9f0f-45e7-aab7-3618c9b554e9;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.7.5 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.1.0 in central\n",
      "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
      "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 in central\n",
      "\tfound com.101tec#zkclient;0.3 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      ":: resolution report :: resolve 4167ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.101tec#zkclient;0.3 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.1.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.7.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.16 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\torg.apache.kafka#kafka-clients;0.8.2.1 by [org.apache.kafka#kafka-clients;2.4.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   1   |   1   |   2   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tServer access error at url https://dl.bintray.com/spark-packages/maven/org/apache/spark/spark-parent_2.11/2.1.0/spark-parent_2.11-2.1.0.jar (javax.net.ssl.SSLHandshakeException: Remote host terminated the handshake)\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1e6b4e8d-9f0f-45e7-aab7-3618c9b554e9\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/9ms)\n",
      "22/11/20 15:41:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(f\"Crawl-test\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b41a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BROKER_1='kafka-1:9092'\n",
    "KAFKA_BROKER_2='kafka-2:9092'\n",
    "KAFKA_BROKER_3='kafka-3:9092'\n",
    "\n",
    "KAFKA_TOPIC= 'ShopInfo'\n",
    "kafkaMessages = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", f\"{KAFKA_BROKER_1},{KAFKA_BROKER_2},{KAFKA_BROKER_3}\") \\\n",
    "  .option(\"subscribe\", KAFKA_TOPIC) \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .load()\n",
    "\n",
    "message = kafkaMessages.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "fileStream = message.writeStream \\\n",
    "                  .trigger(processingTime='300 seconds')\\\n",
    "                  .queryName(\"Persist the processed data\") \\\n",
    "                  .outputMode(\"append\") \\\n",
    "                  .format(\"parquet\") \\\n",
    "                  .option(\"path\", f\"hdfs://namenode:9000/tiki_test/{KAFKA_TOPIC}\") \\\n",
    "                  .option(\"checkpointLocation\", f\"hdfs://namenode:9000/tiki_test/checkpoints_{KAFKA_TOPIC}\") \\\n",
    "                  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d151ddbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data inseart in Kafka -> Stop spark...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if fileStream.lastProgress != None:\n",
    "        if fileStream.lastProgress['numInputRows'] == 0:\n",
    "            print('No data inseart in Kafka -> Stop spark...')\n",
    "            fileStream.stop()\n",
    "            spark.stop()\n",
    "            break\n",
    "        else:\n",
    "            print('Last processing item: ', fileStream.lastProgress['numInputRows'])\n",
    "            time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec916a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
