{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a7addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyspark.sql.functions import col,from_json,udf,split,explode,lit,array,lower\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,MapType,FloatType,ArrayType\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "import pickle\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2586761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,CrossValidatorModel\n",
    "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96d43bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 13:47:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"ml\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1024m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5731827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = {}\n",
    "        self.load_model()\n",
    "        self.load_dictionary()\n",
    "        self.load_stop_word()\n",
    "    \n",
    "    def load_model(self):\n",
    "        list_model = ['lr_yes','lr_no','rf_yes','rf_no']\n",
    "        for model_name in list_model:\n",
    "            self.model[model_name] = CrossValidatorModel.load(f'hdfs://namenode:9000/save_model/{model_name}')\n",
    "            \n",
    "        self.model_tfidf = PipelineModel.load(f'hdfs://namenode:9000/save_model/model_tfidf')\n",
    "    \n",
    "    def load_stop_word(self):\n",
    "        with open('data/dict_stop_word.pkl', 'rb') as f:\n",
    "            self.dict_stop_word = pickle.load(f)\n",
    "            \n",
    "        \n",
    "    def load_dictionary(self):\n",
    "        pst_word = {}\n",
    "        ngt_word = {}\n",
    "        with open('vi_sentiment/positive_words_vi.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.replace('\\n','')\n",
    "                if line not in pst_word:\n",
    "                    pst_word[line] = 1\n",
    "        with open('vi_sentiment/negative_words_vi.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.replace('\\n','')\n",
    "                if line not in pst_word:\n",
    "                    ngt_word[line] = 1\n",
    "        self.pst_word = pst_word\n",
    "        self.ngt_word = ngt_word\n",
    "        \n",
    "    \n",
    "    def cleanText(self,str_raw):\n",
    "        CLEANR = re.compile('<.*?>') \n",
    "        # remove tags html\n",
    "        str_raw = re.sub(CLEANR, ' ', str_raw)\n",
    "\n",
    "        # remove special character\n",
    "        str_raw = re.sub('\\W+', ' ', str_raw)\n",
    "\n",
    "        # remove number\n",
    "        str_raw = re.sub(\"[0-9]+\", \"\", str_raw)\n",
    "\n",
    "        # remove space\n",
    "        cleantext = re.sub(\" +\", \" \", str_raw)\n",
    "        return cleantext.lower()\n",
    "    \n",
    "    \n",
    "    def rule_lexicon_based(self,sentent):\n",
    "        list_token = sentent.split(' ')\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        for token in list_token:\n",
    "            if token in self.pst_word:\n",
    "                pos += 1\n",
    "            elif token in self.ngt_word:\n",
    "                neg += 1\n",
    "        score = pos - neg\n",
    "        if score > 0:\n",
    "            return 2\n",
    "        elif score == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def predict(self, txt, model_name='lr',mode='yes'):\n",
    "        dict_stop_word = self.dict_stop_word\n",
    "        \n",
    "        def remove_stop_word(txt):\n",
    "            txt = txt.strip()\n",
    "            ls_words = txt.split()\n",
    "            ls_new_words = []\n",
    "            for word in ls_words:\n",
    "                if dict_stop_word.get(word) == None:\n",
    "                    ls_new_words.append(word)\n",
    "            return ' '.join(ls_new_words)\n",
    "    \n",
    "        clean_text = self.cleanText(txt)\n",
    "        data = [(clean_text,)]\n",
    "        schema = StructType([ \\\n",
    "            StructField(\"clean_content\",StringType(),True),\n",
    "          ])\n",
    "        \n",
    "        input_data = spark.createDataFrame(data=data,schema=schema)\n",
    "        input_data.createOrReplaceTempView('input_data')\n",
    "        spark.udf.register(\"remove_stop_word\", remove_stop_word,StringType())\n",
    "        \n",
    "        input_data = spark.sql(\"\"\"\n",
    "            select remove_stop_word(clean_content) clean_content\n",
    "            from input_data\n",
    "        \"\"\")\n",
    "        \n",
    "        input_data = input_data.select(split(input_data.clean_content, ' ').alias('cmt_token'))\n",
    "        input_idf = model.model_tfidf.transform(input_data)\n",
    "        \n",
    "        \n",
    "        if model_name == 'lr':\n",
    "            if mode == 'yes':\n",
    "                result = self.model['lr_yes'].transform(input_idf).select('prediction').take(1)\n",
    "            else:\n",
    "                result = self.model['lr_no'].transform(input_idf).select('prediction').take(1)\n",
    "        elif model_name == 'rf':\n",
    "            if mode == 'yes':\n",
    "                result = self.model['rf_yes'].transform(input_idf).select('prediction').take(1)\n",
    "            else:\n",
    "                result = self.model['rf_no'].transform(input_idf).select('prediction').take(1)\n",
    "        elif model_name == 'rule_based':\n",
    "            result = self.rule_lexicon_based(clean_text)\n",
    "        \n",
    "        if model_name != 'rule_based':\n",
    "            result = result[0].prediction\n",
    "            \n",
    "        print('Input data:', txt)\n",
    "        print('='*40)\n",
    "        print('Prediction: ')\n",
    "        if result == 2:\n",
    "            print('Positive')\n",
    "        elif result == 1:\n",
    "            print('Neural')\n",
    "        else:\n",
    "            print('Negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dde421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36982b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'không đẹp lắm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1d32424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 13:49:29 WARN SimpleFunctionRegistry: The function remove_stop_word replaced a previously registered function.\n",
      "23/02/14 13:49:29 WARN DAGScheduler: Broadcasting large task binary with size 1097.1 KiB\n",
      "23/02/14 13:49:29 WARN DAGScheduler: Broadcasting large task binary with size 1097.1 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: không đẹp lắm\n",
      "========================================\n",
      "Prediction: \n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "model.predict(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "929494b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 13:49:31 WARN SimpleFunctionRegistry: The function remove_stop_word replaced a previously registered function.\n",
      "23/02/14 13:49:31 WARN DAGScheduler: Broadcasting large task binary with size 1097.0 KiB\n",
      "23/02/14 13:49:31 WARN DAGScheduler: Broadcasting large task binary with size 1097.0 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: không đẹp lắm\n",
      "========================================\n",
      "Prediction: \n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "model.predict(txt,'lr','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fcd84f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 13:49:32 WARN SimpleFunctionRegistry: The function remove_stop_word replaced a previously registered function.\n",
      "23/02/14 13:49:32 WARN DAGScheduler: Broadcasting large task binary with size 1000.3 KiB\n",
      "23/02/14 13:49:32 WARN DAGScheduler: Broadcasting large task binary with size 1000.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: không đẹp lắm\n",
      "========================================\n",
      "Prediction: \n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "model.predict(txt,'rf','yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6dfe8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 13:49:32 WARN SimpleFunctionRegistry: The function remove_stop_word replaced a previously registered function.\n",
      "23/02/14 13:49:32 WARN DAGScheduler: Broadcasting large task binary with size 1022.2 KiB\n",
      "23/02/14 13:49:33 WARN DAGScheduler: Broadcasting large task binary with size 1022.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: không đẹp lắm\n",
      "========================================\n",
      "Prediction: \n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "model.predict(txt,'rf','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "906747dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: không đẹp lắm\n",
      "========================================\n",
      "Prediction: \n",
      "Neural\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 13:49:33 WARN SimpleFunctionRegistry: The function remove_stop_word replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "model.predict(txt,'rule_based','no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155793b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6754208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da90e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
