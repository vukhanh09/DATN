{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6af7cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyspark.sql.functions import col,from_json,udf,split,explode,lit,array,lower\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,MapType,FloatType,ArrayType\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a2a223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c01a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/19 11:09:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"ml\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"2048m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dac344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.df_test = spark.read.parquet('hdfs://namenode:9000/ml/test_data')\n",
    "        self.df_train = spark.read.parquet('hdfs://namenode:9000/ml/train_data')\n",
    "        self.clean_data()\n",
    "        self.split_content()\n",
    "        self.convert_feature()\n",
    "        self.model = {}\n",
    "        \n",
    "    def getNGram(self,df,n):\n",
    "        ngram = NGram(n=n)\n",
    "        ngram.setInputCol(\"comment_term\")\n",
    "        ngram.setOutputCol(\"nGrams\")\n",
    "        df_nGram = ngram.transform(df)\n",
    "        result_nGram = df_nGram.withColumn('word',explode(df_nGram.nGrams))\\\n",
    "            .groupBy(['word'])\\\n",
    "            .count()\n",
    "        return result_nGram\n",
    "        \n",
    "    def clean_data(self):\n",
    "    \n",
    "        df = self.df_train.withColumn('comment_term',split(self.df_train.clean_content, ' ', -1))\n",
    "\n",
    "        result_nGram = self.getNGram(df,1)\n",
    "        result_nGram.createOrReplaceTempView('result_nGram')\n",
    "        \n",
    "        stop_word = spark.sql(\"\"\"\n",
    "            select word from result_nGram\n",
    "            where count < 10\n",
    "        \"\"\").toPandas()\n",
    "        stop_word = stop_word['word'].to_list()\n",
    "        \n",
    "        dict_stop_word = {x:1 for x in stop_word}\n",
    "        self.dict_stop_word = dict_stop_word\n",
    "        self.df_test.createOrReplaceTempView('df_test')\n",
    "        self.df_train.createOrReplaceTempView('df_train')\n",
    "        \n",
    "        \n",
    "        def remove_stop_word(txt):\n",
    "            txt = txt.strip()\n",
    "            ls_words = txt.split()\n",
    "            ls_new_words = []\n",
    "            for word in ls_words:\n",
    "                if dict_stop_word.get(word) == None:\n",
    "                    ls_new_words.append(word)\n",
    "            return ' '.join(ls_new_words)\n",
    "        spark.udf.register(\"remove_stop_word\", remove_stop_word,StringType())\n",
    "        \n",
    "        self.df_test = spark.sql(\"\"\"\n",
    "            select remove_stop_word(clean_content) clean_content,rating,sentiment,true_label,label \n",
    "            from df_test\n",
    "        \"\"\")\n",
    "        \n",
    "        self.df_train = spark.sql(\"\"\"\n",
    "            select remove_stop_word(clean_content) clean_content,rating,sentiment,label \n",
    "            from df_train\n",
    "        \"\"\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def set_weight(self, w_a = 5,w_b = 5, w_c = 1):\n",
    "        class_weights_spark = {0:w_a,1:w_b,2:w_c}\n",
    "        mapping_expr = F.create_map([F.lit(x) for x in chain(*class_weights_spark.items())])\n",
    "        self.train_idf = self.train_idf.withColumn(\"weight\", mapping_expr.getItem(F.col(\"label\")))\n",
    "        \n",
    "    def split_content(self):\n",
    "        self.train_set = self.df_train.select(split(self.df_train.clean_content, ' ').alias('cmt_token'),'clean_content','rating', 'label')\n",
    "        self.test_set = self.df_test.select(split(self.df_test.clean_content, ' ').alias('cmt_token'),'clean_content','rating', 'label','true_label')\n",
    "    \n",
    "    def convert_feature(self):\n",
    "        count = CountVectorizer(inputCol=\"cmt_token\", outputCol=\"rawFeatures\")\n",
    "        idf = IDF(inputCol=\"rawFeatures\", outputCol=\"featuresTFIDF\")\n",
    "        pipeline = Pipeline(stages=[count, idf])\n",
    "        self.model_tfidf = pipeline.fit(self.train_set)\n",
    "        self.train_idf = self.model_tfidf.transform(self.train_set)\n",
    "        self.test_idf = self.model_tfidf.transform(self.test_set)\n",
    "    \n",
    "    def model_logistic(self,weight):\n",
    "        if weight == True:\n",
    "            lr = LogisticRegression(maxIter=20,featuresCol = \"featuresTFIDF\", tol=1E-6,regParam=0.3, elasticNetParam=0,weightCol=\"weight\")\n",
    "        else:\n",
    "            lr = LogisticRegression(maxIter=20,featuresCol = \"featuresTFIDF\", tol=1E-6,regParam=0.3, elasticNetParam=0)\n",
    "        \n",
    "        paramGrid = ParamGridBuilder()\\\n",
    "                    .addGrid(lr.maxIter, [10, 20, 50])\\\n",
    "                    .addGrid(lr.regParam, [0.1,0.3,0.5])\\\n",
    "                    .addGrid(lr.elasticNetParam,  [0.0, 0.1, 0.2])\\\n",
    "                    .build()\n",
    "        evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "        crossval = CrossValidator(estimator=lr,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=5) \n",
    "        model = crossval.fit(self.train_idf)\n",
    "        if weight == True:\n",
    "            self.model['lr_yes'] = model\n",
    "        else:\n",
    "            self.model['lr_no'] = model\n",
    "        predictions = model.transform(self.test_idf)\n",
    "        return predictions\n",
    "    \n",
    "    def model_rf(self,weight):\n",
    "        if weight == True:\n",
    "            trainer = RandomForestClassifier(featuresCol = \"featuresTFIDF\",weightCol=\"weight\")\n",
    "        else:\n",
    "            trainer = RandomForestClassifier(featuresCol = \"featuresTFIDF\")\n",
    "            \n",
    "        paramGrid = ParamGridBuilder()\\\n",
    "                .addGrid(trainer.numTrees, [10,20,50])\\\n",
    "               .addGrid(trainer.maxDepth, [2,6,8])\\\n",
    "               .addGrid(trainer.minInstancesPerNode, [1,3,5])\\\n",
    "               .build()\n",
    "        \n",
    "        evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "        crossval = CrossValidator(estimator=trainer,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=5) \n",
    "        model = crossval.fit(self.train_idf)\n",
    "        if weight == True:\n",
    "            self.model['rf_yes'] = model\n",
    "        else:\n",
    "            self.model['rf_no'] = model\n",
    "        predictions = model.transform(self.test_idf)\n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self,predictions):\n",
    "        result = predictions.select('true_label', 'prediction')\n",
    "        result = result[['true_label','prediction']].toPandas()\n",
    "        \n",
    "        print(f'accuracy_score: ',accuracy_score(result.true_label, result.prediction))\n",
    "        print(f'prediction: ',precision_score(result.true_label, result.prediction, average='weighted'))\n",
    "        print(f'recall_score: ',recall_score(result.true_label, result.prediction, average='weighted'))\n",
    "        print(f'f1_score: ',f1_score(result.true_label, result.prediction, average='weighted'))\n",
    "        print(classification_report(result.true_label, result.prediction))\n",
    "        \n",
    "    def save_model(self):\n",
    "        list_model = ['lr_yes','lr_no','rf_yes','rf_no']\n",
    "        for model_name in list_model:\n",
    "            self.model[model_name].write().overwrite().save(f'hdfs://namenode:9000/save_model/{model_name}')\n",
    "        \n",
    "        self.model_tfidf.write().overwrite().save(f'hdfs://namenode:9000/save_model/model_tfidf')\n",
    "        \n",
    "        with open('data/dict_stop_word.pkl', 'wb') as f:\n",
    "            pickle.dump(self.dict_stop_word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21a045c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = SentimentModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b3de18",
   "metadata": {},
   "source": [
    "# Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f40fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb0_cnt = model.train_set.filter(col('label') == 0).count()\n",
    "lb1_cnt = model.train_set.filter(col('label') == 1).count()\n",
    "lb2_cnt = model.train_set.filter(col('label') == 2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b378083",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_a = int(lb2_cnt/lb0_cnt)\n",
    "w_b = int(lb2_cnt/lb1_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c488f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weight(w_a,w_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4315d9bc",
   "metadata": {},
   "source": [
    "# LogicRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5c47eb",
   "metadata": {},
   "source": [
    "## Weight balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee597d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_wb = model.model_logistic(weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a11167cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/18 17:01:05 WARN DAGScheduler: Broadcasting large task binary with size 1167.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.8245931283905967\n",
      "prediction:  0.8410244847882596\n",
      "recall_score:  0.8245931283905967\n",
      "f1_score:  0.8317050234923122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74       805\n",
      "           1       0.40      0.50      0.44       538\n",
      "           2       0.93      0.89      0.91      3634\n",
      "\n",
      "    accuracy                           0.82      4977\n",
      "   macro avg       0.68      0.71      0.70      4977\n",
      "weighted avg       0.84      0.82      0.83      4977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(predictions_wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6f864",
   "metadata": {},
   "source": [
    "## No balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd797c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_no_wb = model.model_logistic(weight=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02d23331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/18 17:45:37 WARN DAGScheduler: Broadcasting large task binary with size 1167.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.8111312035362668\n",
      "prediction:  0.778714701546379\n",
      "recall_score:  0.8111312035362668\n",
      "f1_score:  0.7603358919740008\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.52      0.65       805\n",
      "           1       0.45      0.03      0.05       538\n",
      "           2       0.81      0.99      0.89      3634\n",
      "\n",
      "    accuracy                           0.81      4977\n",
      "   macro avg       0.71      0.51      0.53      4977\n",
      "weighted avg       0.78      0.81      0.76      4977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(predictions_no_wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64c6748",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9dc60e",
   "metadata": {},
   "source": [
    "## Weight balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4df728",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions_wb = model.model_rf(weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56ab263e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/19 03:29:54 WARN DAGScheduler: Broadcasting large task binary with size 1390.0 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.7862165963431786\n",
      "prediction:  0.7924247681816011\n",
      "recall_score:  0.7862165963431786\n",
      "f1_score:  0.7775392457146962\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.82      0.65       805\n",
      "           1       0.51      0.21      0.30       538\n",
      "           2       0.89      0.86      0.88      3634\n",
      "\n",
      "    accuracy                           0.79      4977\n",
      "   macro avg       0.64      0.63      0.61      4977\n",
      "weighted avg       0.79      0.79      0.78      4977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(rf_predictions_wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82770598",
   "metadata": {},
   "source": [
    "## No balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions_no_wb = model.model_rf(weight=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "728f4477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/19 20:58:07 WARN DAGScheduler: Broadcasting large task binary with size 1065.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.7301587301587301\n",
      "prediction:  0.5331317712270093\n",
      "recall_score:  0.7301587301587301\n",
      "f1_score:  0.6162807630697539\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       805\n",
      "           1       0.00      0.00      0.00       538\n",
      "           2       0.73      1.00      0.84      3634\n",
      "\n",
      "    accuracy                           0.73      4977\n",
      "   macro avg       0.24      0.33      0.28      4977\n",
      "weighted avg       0.53      0.73      0.62      4977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(rf_predictions_no_wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639547a0",
   "metadata": {},
   "source": [
    "# Analysis result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257426d",
   "metadata": {},
   "source": [
    "## Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900b40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c4825",
   "metadata": {},
   "source": [
    "## Best param LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cc230aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_8a1de7e06574', name='maxIter', doc='max number of iterations (>= 0).'): 10,\n",
       " Param(parent='LogisticRegression_8a1de7e06574', name='regParam', doc='regularization parameter (>= 0).'): 0.5,\n",
       " Param(parent='LogisticRegression_8a1de7e06574', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model['lr_yes'].getEstimatorParamMaps()[ np.argmax(model.model['lr_yes'].avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e61da17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_6cd7ff919380', name='maxIter', doc='max number of iterations (>= 0).'): 20,\n",
       " Param(parent='LogisticRegression_6cd7ff919380', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       " Param(parent='LogisticRegression_6cd7ff919380', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model['lr_no'].getEstimatorParamMaps()[ np.argmax(model.model['lr_no'].avgMetrics) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de99040",
   "metadata": {},
   "source": [
    "## Best param Rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f495001a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_a43184117d6e', name='numTrees', doc='Number of trees to train (>= 1).'): 50,\n",
       " Param(parent='RandomForestClassifier_a43184117d6e', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 6,\n",
       " Param(parent='RandomForestClassifier_a43184117d6e', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model['rf_yes'].getEstimatorParamMaps()[ np.argmax(model.model['rf_yes'].avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769477b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_e24d6640e138', name='numTrees', doc='Number of trees to train (>= 1).'): 10,\n",
       " Param(parent='RandomForestClassifier_e24d6640e138', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 8,\n",
       " Param(parent='RandomForestClassifier_e24d6640e138', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 5}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model['rf_no'].getEstimatorParamMaps()[ np.argmax(model.model['rf_no'].avgMetrics) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889e494",
   "metadata": {},
   "source": [
    "## Avg accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a418e247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7764573125965952,\n",
       " 0.7456340038571283,\n",
       " 0.694868138187162,\n",
       " 0.7766417513383568,\n",
       " 0.6603483030996044,\n",
       " 0.5699171978957835,\n",
       " 0.7776969799924368,\n",
       " 0.5873499331291506,\n",
       " 0.4123602067998112,\n",
       " 0.774367323242944,\n",
       " 0.7359863382765413,\n",
       " 0.6904136621058743,\n",
       " 0.7723524748217678,\n",
       " 0.6607537375301171,\n",
       " 0.5708691275277076,\n",
       " 0.7726548724826142,\n",
       " 0.5872946996660414,\n",
       " 0.4123602067998112,\n",
       " 0.773169535841704,\n",
       " 0.7351117228190561,\n",
       " 0.6904737189325507,\n",
       " 0.772075860998275,\n",
       " 0.6607463578471462,\n",
       " 0.5708691275277076,\n",
       " 0.7725931431128752,\n",
       " 0.5872928558127568,\n",
       " 0.4123602067998112]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model['lr_yes'].avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c67d11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8427689582216817,\n",
       " 0.813631009358676,\n",
       " 0.8008990360347747,\n",
       " 0.8270214764026717,\n",
       " 0.7934305370142724,\n",
       " 0.7923292488967033,\n",
       " 0.8175054003187974,\n",
       " 0.792344920769156,\n",
       " 0.7923329303378015,\n",
       " 0.8438214947877416,\n",
       " 0.8138266244500139,\n",
       " 0.8007748666813412,\n",
       " 0.8276427198694821,\n",
       " 0.7934508287094659,\n",
       " 0.7923292488967033,\n",
       " 0.8178805210158259,\n",
       " 0.792344920769156,\n",
       " 0.7923329303378015,\n",
       " 0.8436574566525537,\n",
       " 0.8138431540096609,\n",
       " 0.8007260000977876,\n",
       " 0.8275809224566997,\n",
       " 0.7934425264113083,\n",
       " 0.7923292488967033,\n",
       " 0.8178712927022566,\n",
       " 0.792344920769156,\n",
       " 0.7923329303378015]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model['lr_no'].avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a855c85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7138675397647222,\n",
       " 0.7138675397647222,\n",
       " 0.7138675397647222,\n",
       " 0.7465267035308213,\n",
       " 0.7199575766075282,\n",
       " 0.7470936372750179,\n",
       " 0.7416897688375652,\n",
       " 0.7295085577071947,\n",
       " 0.7299894681347712,\n",
       " 0.718955712961225,\n",
       " 0.718955712961225,\n",
       " 0.718955712961225,\n",
       " 0.7361737424047989,\n",
       " 0.7616374644691588,\n",
       " 0.7269110658394248,\n",
       " 0.742663707901961,\n",
       " 0.7503309425088778,\n",
       " 0.7469322706227179,\n",
       " 0.7640029393007719,\n",
       " 0.7640029393007719,\n",
       " 0.7640020204364881,\n",
       " 0.7810081686184898,\n",
       " 0.7769529385892355,\n",
       " 0.7786959499214202,\n",
       " 0.7775047554105232,\n",
       " 0.776793100003014,\n",
       " 0.7803226995833168]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/19 06:48:56 WARN DAGScheduler: Broadcasting large task binary with size 1327.9 KiB\n",
      "[Stage 11764:>                                                      (0 + 2) / 3]\r"
     ]
    }
   ],
   "source": [
    "model.model['rf_yes'].avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d02f94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.792333505755006,\n",
       " 0.792333505755006,\n",
       " 0.792333505755006,\n",
       " 0.7923371923971239,\n",
       " 0.7923353520849636,\n",
       " 0.7923390293951313,\n",
       " 0.7923998968037614,\n",
       " 0.7923869661639318,\n",
       " 0.7924026191211583,\n",
       " 0.792333505755006,\n",
       " 0.792333505755006,\n",
       " 0.792333505755006,\n",
       " 0.7923344267264467,\n",
       " 0.792334427528683,\n",
       " 0.792333505755006,\n",
       " 0.792340873581691,\n",
       " 0.792344566588896,\n",
       " 0.7923390355207991,\n",
       " 0.792333505755006,\n",
       " 0.792333505755006,\n",
       " 0.792333505755006,\n",
       " 0.792333505755006,\n",
       " 0.792333505755006,\n",
       " 0.792333505755006,\n",
       " 0.7923399478496358,\n",
       " 0.7923418072594139,\n",
       " 0.7923399601272201]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model['rf_no'].avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95292379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
