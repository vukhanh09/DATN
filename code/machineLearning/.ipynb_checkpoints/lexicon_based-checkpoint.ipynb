{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b169e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyspark.sql.functions import col,from_json,udf,split,explode,lit,array\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,MapType,FloatType,ArrayType\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b77f444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/17 13:21:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/17 13:21:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"ml\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1024m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec2de9",
   "metadata": {},
   "source": [
    "# Load raw comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b90043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.parquet('hdfs://namenode:9000/TikiCleaned/Comment')\n",
    "data.createOrReplaceTempView('data')\n",
    "df = spark.sql(\"\"\"\n",
    "    select distinct clean_content,rating,sentiment,\n",
    "    case\n",
    "        when rating >= 4 then 2\n",
    "        when rating = 3 then 1\n",
    "        else 0\n",
    "    end as label\n",
    "    from data\n",
    "    where clean_content <> '' and clean_content is not null and clean_content <> ' '\n",
    "\"\"\")\n",
    "df.toPandas().to_csv('data/data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e3046",
   "metadata": {},
   "source": [
    "# Load labeling comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3791df25",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv')\n",
    "train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b1bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['true_label'] = test['true_label'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaef90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.createDataFrame(test)\n",
    "df_train = spark.createDataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c609976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_test.write.partitionBy(\"label\").mode('overwrite').parquet('hdfs://namenode:9000/ml/test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c883f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/17 13:22:48 WARN TaskSetManager: Stage 4 contains a task of very large size (60802 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train.write.partitionBy(\"label\").mode('overwrite').parquet('hdfs://namenode:9000/ml/train_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ecb05e",
   "metadata": {},
   "source": [
    "## Lexicon-based sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b15407e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.withColumn('comment_term',split(df_train.clean_content, ' ', -1))\n",
    "def getNGram(n):\n",
    "    ngram = NGram(n=n)\n",
    "    ngram.setInputCol(\"comment_term\")\n",
    "    ngram.setOutputCol(\"nGrams\")\n",
    "    df_nGram = ngram.transform(df)\n",
    "    result_nGram = df_nGram.withColumn('word',explode(df_nGram.nGrams))\\\n",
    "        .groupBy(['label','word'])\\\n",
    "        .count()\n",
    "    return result_nGram\n",
    "result_nGram = getNGram(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc3dd432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/17 13:23:23 WARN TaskSetManager: Stage 5 contains a task of very large size (60802 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/01/17 13:23:35 WARN TaskSetManager: Stage 10 contains a task of very large size (60802 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/01/17 13:23:43 WARN TaskSetManager: Stage 15 contains a task of very large size (60802 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    result_nGram.filter(result_nGram.label == i).orderBy(col(\"count\").desc()).toPandas().to_csv(f'sample/{i}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0423cf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_word = {}\n",
    "ngt_word = {}\n",
    "with open('vi_sentiment/positive_words_vi.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n','')\n",
    "        if line not in pst_word:\n",
    "            pst_word[line] = 1\n",
    "with open('vi_sentiment/negative_words_vi.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n','')\n",
    "        if line not in pst_word:\n",
    "            ngt_word[line] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bc6a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(sentent):\n",
    "    list_token = sentent.split(' ')\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for token in list_token:\n",
    "        if token in pst_word:\n",
    "            pos += 1\n",
    "        elif token in ngt_word:\n",
    "            neg += 1\n",
    "    score = pos - neg\n",
    "    if score > 0:\n",
    "        return 2\n",
    "    elif score == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d19c5f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.prediction(sentent)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"prediction\", prediction,IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc0696cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.createOrReplaceTempView(\"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "506e2fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = spark.sql(\"\"\"\n",
    "    select true_label,label,prediction(clean_content) prediction from test_data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "714a72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test =  result_test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9113a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.7217199115933293\n",
      "prediction:  0.7081688945766338\n",
      "recall_score:  0.7217199115933293\n",
      "f1_score:  0.7142375841404099\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy_score: ',accuracy_score(result_test.true_label, result_test.prediction))\n",
    "print(f'prediction: ',precision_score(result_test.true_label, result_test.prediction, average='weighted'))\n",
    "print(f'recall_score: ',recall_score(result_test.true_label, result_test.prediction, average='weighted'))\n",
    "print(f'f1_score: ',f1_score(result_test.true_label, result_test.prediction, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b52ea9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.43      0.46       805\n",
      "           1       0.15      0.13      0.14       538\n",
      "           2       0.84      0.87      0.86      3634\n",
      "\n",
      "    accuracy                           0.72      4977\n",
      "   macro avg       0.49      0.48      0.49      4977\n",
      "weighted avg       0.71      0.72      0.71      4977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(result_test.true_label, result_test.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2849588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
