{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b23e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyspark.sql.functions import col,from_json,udf,split,explode,lit,array,lower\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,MapType,FloatType,ArrayType\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import chain\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58de2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder,CrossValidatorModel\n",
    "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,f1_score\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50eb62ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 13:53:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"ml\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"1024m\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ec16e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.df_test = spark.read.parquet('hdfs://namenode:9000/ml/test_data')\n",
    "        self.df_train = spark.read.parquet('hdfs://namenode:9000/ml/train_data')\n",
    "        self.clean_data()\n",
    "        self.model = {}\n",
    "        self.load_model()\n",
    "        self.split_content()\n",
    "        self.convert_feature()\n",
    "\n",
    "        \n",
    "        \n",
    "    def load_model(self):\n",
    "        list_model = ['lr_yes','lr_no','rf_yes','rf_no']\n",
    "        for model_name in list_model:\n",
    "            self.model[model_name] = CrossValidatorModel.load(f'hdfs://namenode:9000/save_model/{model_name}')\n",
    "            \n",
    "        self.model_tfidf = PipelineModel.load(f'hdfs://namenode:9000/save_model/model_tfidf')\n",
    "        \n",
    "    def getNGram(self,df,n):\n",
    "        ngram = NGram(n=n)\n",
    "        ngram.setInputCol(\"comment_term\")\n",
    "        ngram.setOutputCol(\"nGrams\")\n",
    "        df_nGram = ngram.transform(df)\n",
    "        result_nGram = df_nGram.withColumn('word',explode(df_nGram.nGrams))\\\n",
    "            .groupBy(['word'])\\\n",
    "            .count()\n",
    "        return result_nGram\n",
    "        \n",
    "    def clean_data(self):\n",
    "    \n",
    "        df = self.df_train.withColumn('comment_term',split(self.df_train.clean_content, ' ', -1))\n",
    "\n",
    "        result_nGram = self.getNGram(df,1)\n",
    "        result_nGram.createOrReplaceTempView('result_nGram')\n",
    "        \n",
    "        stop_word = spark.sql(\"\"\"\n",
    "            select word from result_nGram\n",
    "            where count < 10\n",
    "        \"\"\").toPandas()\n",
    "        stop_word = stop_word['word'].to_list()\n",
    "        \n",
    "        dict_stop_word = {x:1 for x in stop_word}\n",
    "        self.dict_stop_word = dict_stop_word\n",
    "        self.df_test.createOrReplaceTempView('df_test')\n",
    "        self.df_train.createOrReplaceTempView('df_train')\n",
    "        \n",
    "        \n",
    "        def remove_stop_word(txt):\n",
    "            txt = txt.strip()\n",
    "            ls_words = txt.split()\n",
    "            ls_new_words = []\n",
    "            for word in ls_words:\n",
    "                if dict_stop_word.get(word) == None:\n",
    "                    ls_new_words.append(word)\n",
    "            return ' '.join(ls_new_words)\n",
    "        spark.udf.register(\"remove_stop_word\", remove_stop_word,StringType())\n",
    "        \n",
    "        self.df_test = spark.sql(\"\"\"\n",
    "            select remove_stop_word(clean_content) clean_content,rating,sentiment,true_label,label \n",
    "            from df_test\n",
    "        \"\"\")\n",
    "        \n",
    "        self.df_train = spark.sql(\"\"\"\n",
    "            select remove_stop_word(clean_content) clean_content,rating,sentiment,label \n",
    "            from df_train\n",
    "        \"\"\")\n",
    "        \n",
    "\n",
    "    \n",
    "    def set_weight(self, w_a = 5,w_b = 5, w_c = 1):\n",
    "        class_weights_spark = {0:w_a,1:w_b,2:w_c}\n",
    "        mapping_expr = F.create_map([F.lit(x) for x in chain(*class_weights_spark.items())])\n",
    "        self.train_idf = self.train_idf.withColumn(\"weight\", mapping_expr.getItem(F.col(\"label\")))\n",
    "        \n",
    "    def split_content(self):\n",
    "        self.train_set = self.df_train.select(split(self.df_train.clean_content, ' ').alias('cmt_token'),'clean_content','rating', 'label')\n",
    "        self.test_set = self.df_test.select(split(self.df_test.clean_content, ' ').alias('cmt_token'),'clean_content','rating', 'label','true_label')\n",
    "    \n",
    "    def convert_feature(self):\n",
    "        self.train_idf = self.model_tfidf.transform(self.train_set)\n",
    "        self.test_idf = self.model_tfidf.transform(self.test_set)\n",
    "    \n",
    "   \n",
    "    \n",
    "    def evaluate(self,predictions):\n",
    "        result = predictions.select('true_label', 'prediction')\n",
    "        result = result[['true_label','prediction']].toPandas()\n",
    "        \n",
    "        print(f'accuracy_score: ',accuracy_score(result.true_label, result.prediction))\n",
    "        print(f'prediction: ',precision_score(result.true_label, result.prediction, average='weighted'))\n",
    "        print(f'recall_score: ',recall_score(result.true_label, result.prediction, average='weighted'))\n",
    "        print(f'f1_score: ',f1_score(result.true_label, result.prediction, average='weighted'))\n",
    "        print(classification_report(result.true_label, result.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ab2b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = SentimentModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc32866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 13:53:50 WARN DAGScheduler: Broadcasting large task binary with size 1100.2 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Model: lr_yes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.65      0.71       805\n",
      "           1       0.36      0.54      0.43       538\n",
      "           2       0.93      0.90      0.91      3634\n",
      "\n",
      "    accuracy                           0.82      4977\n",
      "   macro avg       0.69      0.70      0.68      4977\n",
      "weighted avg       0.85      0.82      0.83      4977\n",
      "\n",
      "[[ 520  239   46]\n",
      " [  52  293  193]\n",
      " [  91  289 3254]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 13:53:52 WARN DAGScheduler: Broadcasting large task binary with size 1100.2 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Model: lr_no\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.52      0.65       805\n",
      "           1       0.41      0.02      0.04       538\n",
      "           2       0.81      0.99      0.89      3634\n",
      "\n",
      "    accuracy                           0.81      4977\n",
      "   macro avg       0.70      0.51      0.53      4977\n",
      "weighted avg       0.78      0.81      0.76      4977\n",
      "\n",
      "[[ 415   13  377]\n",
      " [  38   12  488]\n",
      " [  21    4 3609]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 13:53:53 WARN DAGScheduler: Broadcasting large task binary with size 1003.4 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Model: rf_yes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.03      0.06       805\n",
      "           1       0.27      0.21      0.24       538\n",
      "           2       0.77      0.96      0.86      3634\n",
      "\n",
      "    accuracy                           0.73      4977\n",
      "   macro avg       0.68      0.40      0.38      4977\n",
      "weighted avg       0.75      0.73      0.66      4977\n",
      "\n",
      "[[  23  162  620]\n",
      " [   0  111  427]\n",
      " [   0  132 3502]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 13:53:54 WARN DAGScheduler: Broadcasting large task binary with size 1025.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Model: rf_no\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       805\n",
      "           1       0.00      0.00      0.00       538\n",
      "           2       0.73      1.00      0.84      3634\n",
      "\n",
      "    accuracy                           0.73      4977\n",
      "   macro avg       0.58      0.33      0.28      4977\n",
      "weighted avg       0.69      0.73      0.62      4977\n",
      "\n",
      "[[   1    0  804]\n",
      " [   0    0  538]\n",
      " [   0    0 3634]]\n"
     ]
    }
   ],
   "source": [
    "list_model = ['lr_yes','lr_no','rf_yes','rf_no']\n",
    "for model_name in list_model:\n",
    "    res = model.model[model_name].transform(model.test_idf).select(['true_label','prediction'])\n",
    "    y_true = res.select(['true_label']).collect()\n",
    "    y_pred = res.select(['prediction']).collect()\n",
    "    print('='*30)\n",
    "    print('Model:',model_name)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d45a7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.6762339\n",
      "70.6762339\n",
      "70.6762339\n",
      "71.9676083\n",
      "72.4317665\n",
      "71.5733107\n",
      "72.3602181\n",
      "71.1134318\n",
      "70.7826626\n",
      "74.3836415\n",
      "74.3836415\n",
      "74.3839491\n",
      "73.8092061\n",
      "74.3962157\n",
      "73.4733337\n",
      "72.9754401\n",
      "73.9171712\n",
      "73.0888359\n",
      "76.8888341\n",
      "76.888834\n",
      "76.8887316\n",
      "74.6391311\n",
      "74.202973\n",
      "75.3568291\n",
      "74.2218713\n",
      "74.5431967\n",
      "74.9455338\n"
     ]
    }
   ],
   "source": [
    "for x in [round(x*100,7) for x in model.model['rf_yes'].avgMetrics]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e0509e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.05\n",
      "79.05\n",
      "79.05\n",
      "79.06\n",
      "79.06\n",
      "79.06\n",
      "79.07\n",
      "79.07\n",
      "79.06\n",
      "79.05\n",
      "79.05\n",
      "79.05\n",
      "79.05\n",
      "79.05\n",
      "79.05\n",
      "79.06\n",
      "79.06\n",
      "79.06\n",
      "79.05\n",
      "79.05\n",
      "79.05\n",
      "79.05\n",
      "79.05\n",
      "79.05\n",
      "79.06\n",
      "79.06\n",
      "79.06\n"
     ]
    }
   ],
   "source": [
    "for x in [round(x*100,2) for x in model.model['rf_no'].avgMetrics]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e5637a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.model['rf_yes'].avgMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b218ef21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_5eee6cd5b48a', name='weightCol', doc='weight column name. If this is not set or empty, we treat all instance weights as 1.0.'): 'weight',\n",
       " Param(parent='RandomForestClassifier_5eee6cd5b48a', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='RandomForestClassifier_5eee6cd5b48a', name='numTrees', doc='Number of trees to train (>= 1).'): 50,\n",
       " Param(parent='RandomForestClassifier_5eee6cd5b48a', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model['rf_yes'].getEstimatorParamMaps()[ np.argmax(model.model['rf_yes'].avgMetrics) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "368fa6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SentimentModel:\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         self.df_test = spark.read.parquet('hdfs://namenode:9000/ml/test_data')\n",
    "#         self.df_train = spark.read.parquet('hdfs://namenode:9000/ml/train_data')\n",
    "#         self.clean_data()\n",
    "#         self.split_content()\n",
    "#         self.convert_feature()\n",
    "#         self.model = {}\n",
    "        \n",
    "#     def getNGram(self,df,n):\n",
    "#         ngram = NGram(n=n)\n",
    "#         ngram.setInputCol(\"comment_term\")\n",
    "#         ngram.setOutputCol(\"nGrams\")\n",
    "#         df_nGram = ngram.transform(df)\n",
    "#         result_nGram = df_nGram.withColumn('word',explode(df_nGram.nGrams))\\\n",
    "#             .groupBy(['word'])\\\n",
    "#             .count()\n",
    "#         return result_nGram\n",
    "        \n",
    "#     def clean_data(self):\n",
    "    \n",
    "#         df = self.df_train.withColumn('comment_term',split(self.df_train.clean_content, ' ', -1))\n",
    "\n",
    "#         result_nGram = self.getNGram(df,1)\n",
    "#         result_nGram.createOrReplaceTempView('result_nGram')\n",
    "        \n",
    "#         stop_word = spark.sql(\"\"\"\n",
    "#             select word from result_nGram\n",
    "#             where count < 10\n",
    "#         \"\"\").toPandas()\n",
    "#         stop_word = stop_word['word'].to_list()\n",
    "        \n",
    "#         dict_stop_word = {x:1 for x in stop_word}\n",
    "#         self.dict_stop_word = dict_stop_word\n",
    "#         self.df_test.createOrReplaceTempView('df_test')\n",
    "#         self.df_train.createOrReplaceTempView('df_train')\n",
    "        \n",
    "        \n",
    "#         def remove_stop_word(txt):\n",
    "#             txt = txt.strip()\n",
    "#             ls_words = txt.split()\n",
    "#             ls_new_words = []\n",
    "#             for word in ls_words:\n",
    "#                 if dict_stop_word.get(word) == None:\n",
    "#                     ls_new_words.append(word)\n",
    "#             return ' '.join(ls_new_words)\n",
    "#         spark.udf.register(\"remove_stop_word\", remove_stop_word,StringType())\n",
    "        \n",
    "#         self.df_test = spark.sql(\"\"\"\n",
    "#             select remove_stop_word(clean_content) clean_content,rating,sentiment,true_label,label \n",
    "#             from df_test\n",
    "#         \"\"\")\n",
    "        \n",
    "#         self.df_train = spark.sql(\"\"\"\n",
    "#             select remove_stop_word(clean_content) clean_content,rating,sentiment,label \n",
    "#             from df_train\n",
    "#         \"\"\")\n",
    "        \n",
    "\n",
    "    \n",
    "#     def set_weight(self, w_a = 5,w_b = 5, w_c = 1):\n",
    "#         class_weights_spark = {0:w_a,1:w_b,2:w_c}\n",
    "#         mapping_expr = F.create_map([F.lit(x) for x in chain(*class_weights_spark.items())])\n",
    "#         tmp_a = self.train_idf.filter(col('label') == 0)\n",
    "#         tmp_b = self.train_idf.filter(col('label') == 1)\n",
    "#         for i in range(w_a):\n",
    "#             self.train_idf = self.train_idf.unionAll(tmp_a)\n",
    "#         for i in range(w_b):\n",
    "#             self.train_idf = self.train_idf.unionAll(tmp_b)\n",
    "# #         self.train_idf = self.train_idf.withColumn(\"weight\", mapping_expr.getItem(F.col(\"label\")))\n",
    "        \n",
    "#     def split_content(self):\n",
    "#         self.train_set = self.df_train.select(split(self.df_train.clean_content, ' ').alias('cmt_token'),'clean_content','rating', 'label')\n",
    "#         self.test_set = self.df_test.select(split(self.df_test.clean_content, ' ').alias('cmt_token'),'clean_content','rating', 'label','true_label')\n",
    "    \n",
    "#     def convert_feature(self):\n",
    "#         count = CountVectorizer(inputCol=\"cmt_token\", outputCol=\"rawFeatures\")\n",
    "#         idf = IDF(inputCol=\"rawFeatures\", outputCol=\"featuresTFIDF\")\n",
    "#         pipeline = Pipeline(stages=[count, idf])\n",
    "#         self.model_tfidf = pipeline.fit(self.train_set)\n",
    "#         self.train_idf = self.model_tfidf.transform(self.train_set)\n",
    "#         self.test_idf = self.model_tfidf.transform(self.test_set)\n",
    "    \n",
    "#     def model_logistic(self,weight):\n",
    "#         lr = LogisticRegression(featuresCol = \"featuresTFIDF\")\n",
    "\n",
    "#         if weight == True:\n",
    "#             paramGrid = ParamGridBuilder()\\\n",
    "#                         .addGrid(lr.maxIter, [10, 20, 50])\\\n",
    "#                         .addGrid(lr.regParam, [0.1,0.3,0.5])\\\n",
    "#                         .addGrid(lr.weightCol,  [\"weight\"])\\\n",
    "#                         .build()\n",
    "            \n",
    "#         else:\n",
    "#             paramGrid = ParamGridBuilder()\\\n",
    "#                         .addGrid(lr.maxIter, [10, 20, 50])\\\n",
    "#                         .addGrid(lr.regParam, [0.1,0.3,0.5])\\\n",
    "#                         .build()\n",
    "\n",
    "#         evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "#         crossval = CrossValidator(estimator=lr,\n",
    "#                                   estimatorParamMaps=paramGrid,\n",
    "#                                   evaluator=evaluator,\n",
    "#                                   numFolds=5) \n",
    "        \n",
    "#         model = crossval.fit(self.train_idf)\n",
    "#         if weight == True:\n",
    "#             self.model['lr_yes'] = model\n",
    "#         else:\n",
    "#             self.model['lr_no'] = model\n",
    "#         predictions = model.transform(self.test_idf)\n",
    "#         return predictions\n",
    "    \n",
    "#     def model_rf(self,weight):\n",
    "#         trainer = RandomForestClassifier(featuresCol = \"featuresTFIDF\",numTrees=50,minInstancesPerNode=3,maxDepth=10)\n",
    "#         if weight == True:\n",
    "#             paramGrid = ParamGridBuilder()\\\n",
    "#                         .addGrid(trainer.numTrees, [10,20,50])\\\n",
    "#                         .addGrid(trainer.maxDepth, [2,6,8])\\\n",
    "#                         .addGrid(trainer.minInstancesPerNode, [1,3,5])\\\n",
    "#                         .addGrid(trainer.weightCol,  [\"weight\"])\\\n",
    "#                         .build()\n",
    "            \n",
    "#         else:\n",
    "#             paramGrid = ParamGridBuilder()\\\n",
    "#                         .addGrid(trainer.numTrees, [10,20,50])\\\n",
    "#                         .addGrid(trainer.maxDepth, [2,6,8])\\\n",
    "#                         .addGrid(trainer.minInstancesPerNode, [1,3,5])\\\n",
    "#                         .build()\n",
    "        \n",
    "#         evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "#         crossval = CrossValidator(estimator=trainer,\n",
    "#                                   estimatorParamMaps=paramGrid,\n",
    "#                                   evaluator=evaluator,\n",
    "#                                   numFolds=5) \n",
    "#         model = trainer.fit(self.train_idf)\n",
    "#         if weight == True:\n",
    "#             self.model['rf_yes'] = model\n",
    "#         else:\n",
    "#             self.model['rf_no'] = model\n",
    "#         predictions = model.transform(self.test_idf)\n",
    "#         return predictions\n",
    "    \n",
    "#     def evaluate(self,predictions):\n",
    "#         result = predictions.select('true_label', 'prediction')\n",
    "#         result = result[['true_label','prediction']].toPandas()\n",
    "        \n",
    "#         print(f'accuracy_score: ',accuracy_score(result.true_label, result.prediction))\n",
    "#         print(f'prediction: ',precision_score(result.true_label, result.prediction, average='weighted'))\n",
    "#         print(f'recall_score: ',recall_score(result.true_label, result.prediction, average='weighted'))\n",
    "#         print(f'f1_score: ',f1_score(result.true_label, result.prediction, average='weighted'))\n",
    "#         print(classification_report(result.true_label, result.prediction))\n",
    "        \n",
    "#     def save_model(self):\n",
    "#         list_model = ['lr_yes','lr_no','rf_yes','rf_no']\n",
    "#         for model_name in list_model:\n",
    "#             self.model[model_name].write().overwrite().save(f'hdfs://namenode:9000/save_model/{model_name}')\n",
    "        \n",
    "#         self.model_tfidf.write().overwrite().save(f'hdfs://namenode:9000/save_model/model_tfidf')\n",
    "        \n",
    "#         with open('data/dict_stop_word.pkl', 'wb') as f:\n",
    "#             pickle.dump(self.dict_stop_word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8984507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentimentModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4d211f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lb0_cnt = model.train_set.filter(col('label') == 0).count()\n",
    "# lb1_cnt = model.train_set.filter(col('label') == 1).count()\n",
    "# lb2_cnt = model.train_set.filter(col('label') == 2).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ae5c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w_a = int(lb2_cnt/lb0_cnt)\n",
    "# w_b = int(lb2_cnt/lb1_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcc4f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.set_weight(w_a,w_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_predictions_wb = model.model_rf(weight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcc8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate(rf_predictions_wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174a5b2",
   "metadata": {},
   "source": [
    "# Analysis wrong prediction rule_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b525f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.parquet('hdfs://namenode:9000/ml/test_data')\n",
    "df_train = spark.read.parquet('hdfs://namenode:9000/ml/train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8dd2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_test = df_test.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "617dd14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 14:07:18 WARN DAGScheduler: Broadcasting large task binary with size 1101.2 KiB\n",
      "23/02/14 14:07:18 WARN DAGScheduler: Broadcasting large task binary with size 1004.4 KiB\n"
     ]
    }
   ],
   "source": [
    "list_model = ['lr_yes','rf_yes']\n",
    "for model_name in list_model:\n",
    "    res = model.model[model_name].transform(model.test_idf).select(['true_label','prediction'])\n",
    "    res = res.toPandas()\n",
    "    pd_test['prediction'] = res[res['true_label']!=res['prediction']]['prediction']\n",
    "    result = pd_test[pd.notna(pd_test['prediction'])][['true_label','prediction','clean_content']]\n",
    "    result['prediction'] = result['prediction'].astype(int)\n",
    "    result.to_csv(f'wrong_prediction/{model_name}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74f8c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_word = {}\n",
    "ngt_word = {}\n",
    "with open('vi_sentiment/positive_words_vi.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n','')\n",
    "        if line not in pst_word:\n",
    "            pst_word[line] = 1\n",
    "with open('vi_sentiment/negative_words_vi.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n','')\n",
    "        if line not in pst_word:\n",
    "            ngt_word[line] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec15ec78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(sentent):\n",
    "    list_token = sentent.split(' ')\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for token in list_token:\n",
    "        if token in pst_word:\n",
    "            pos += 1\n",
    "        elif token in ngt_word:\n",
    "            neg += 1\n",
    "    score = pos - neg\n",
    "    if score > 0:\n",
    "        return 2\n",
    "    elif score == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "151f650f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.prediction(sentent)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"prediction\", prediction,IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2829731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_anl(sentent):\n",
    "    list_token = sentent.split(' ')\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    pos_token = []\n",
    "    neg_token = []\n",
    "    for token in list_token:\n",
    "        if token in pst_word:\n",
    "            pos_token.append(token)\n",
    "        elif token in ngt_word:\n",
    "            neg_token.append(token)\n",
    "    return [','.join(pos_token),'|'.join(neg_token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "026f1a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/14 14:29:34 WARN SimpleFunctionRegistry: The function token_anl replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.token_anl(sentent)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"token_anl\", token_anl,ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "93828cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.parquet('hdfs://namenode:9000/ml/test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce7fda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.createOrReplaceTempView(\"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7467b43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = spark.sql(\"\"\"\n",
    "    select true_label,prediction(clean_content) prediction,clean_content,token_anl(clean_content) token\n",
    "    from test_data\n",
    "    where prediction(clean_content) != true_label\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c11063c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test.toPandas().to_csv('wrong_prediction/rule_base.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e5b5ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = spark.sql(\"\"\"\n",
    "    select true_label,prediction(clean_content) prediction,clean_content,token_anl(clean_content) token\n",
    "    from test_data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c68a5e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Model: rf_yes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.43      0.46       805\n",
      "           1       0.15      0.13      0.14       538\n",
      "           2       0.84      0.87      0.86      3634\n",
      "\n",
      "    accuracy                           0.72      4977\n",
      "   macro avg       0.49      0.48      0.49      4977\n",
      "weighted avg       0.71      0.72      0.71      4977\n",
      "\n",
      "[[ 346  177  282]\n",
      " [ 134   72  332]\n",
      " [ 214  246 3174]]\n"
     ]
    }
   ],
   "source": [
    "y_true = res.select(['true_label']).collect()\n",
    "y_pred = res.select(['prediction']).collect()\n",
    "print('='*30)\n",
    "print('Model:',model_name)\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
